# Complex fractal trainability boundary can arise from trivial non-convexity 

## General information
This repository contains code and data of [this short paper](https://arxiv.org/abs/2406.13971), where fractal trainability boundaries (in changing learning rate) are found even with very simple non-convex functions. We therefore guess that fractal behaviors generally exist in non-covex optimization based on discrete iterations.

Here is a one dimensional fractal generated based on loss during training, where the x-axis is learning rate and the y-axis has no meaning.
![see figure](./figures/visual.jpg)

".jl" and ".py" in the repository are for experiments, ".ipynb" is for tests and data analysis, and ".mat" contains raw data. Note exp-i.jl and gpuexp-i.py are doing the same experiment. We move the latter to GPUs.

## Fig. 1a
Partly done with visual-1.ipynb

## Fig. 1b
Partly done with visual-0.ipynb

## Fig. 1d
Partly done with visual-0.ipynb and gputest-3.py.

## Fig. 2a
See exp-1.

## Fig. 2b
See exp-0.

## Fig. 2c and Fig. 3a
See gpuexp-4-2.

## Fig. 2d and Fig. 3b
See gpuexp-8-0.

## Fig. 4a
See gpuexp-10.

## Fig. 4b
See exp-5.

## Fig. 4c
See exp-3.

## Fig. S1
See test-0.ipynb.

## Fig. S2
See test-3.ipynb.

## Fig. S3
(a), (b), (c), and (d) are from gpuexp-8, gpuexp-8-1, gpuexp-8-2, and gpuexp-8-3, respectively.

## Fig. S4
See exp-7.

## Fig. S5
See exp-6.
